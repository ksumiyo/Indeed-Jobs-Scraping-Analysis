{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce840912",
   "metadata": {},
   "source": [
    "This notebook utilizes Python to scrape Indeed for job information based on inputs given by the user, then export the dataframe as a csv so that it can be further cleaned and analyzed in a notebook more tailored to the information contained within the dataset. It was coded in the early stages of my project, hence the flexibility of inputting the job position, the extent of search results to scrape, and the keyword(s) to flag job posts for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63423972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries for scraping, cleaning, storing data\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import html5lib\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054b9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Selenium & Webdriver instances\n",
    "service = Service(## PATH TO CHROMEDRIVER.EXE)\n",
    "search_driver = webdriver.Chrome(service = service)\n",
    "keyword_driver = webdriver.Chrome(service = service)\n",
    "salary_driver = webdriver.Chrome(service = service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d0383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job title to search for: data analyst\n",
      "Enter the number of locations to search for the job position: 20\n",
      "Enter each location as city, state (abbreviate state)\n",
      "Enter location #1: austin, tx\n",
      "Enter location #2: san jose, ca\n",
      "Enter location #3: dallas, tx\n",
      "Enter location #4: atlanta, ga\n",
      "Enter location #5: huntsville, al\n",
      "Enter location #6: charlotte, nc\n",
      "Enter location #7: raleigh, nc\n",
      "Enter location #8: san francisco, ca\n",
      "Enter location #9: seattle, wa\n",
      "Enter location #10: washington dc\n",
      "Enter location #11: baltimore, md\n",
      "Enter location #12: boulder, co\n",
      "Enter location #13: colorado springs, co\n",
      "Enter location #14: new york, ny\n",
      "Enter location #15: los angeles, ca\n",
      "Enter location #16: trenton, nj\n",
      "Enter location #17: baltimore, md\n",
      "Enter location #18: lansing, mi\n",
      "Enter location #19: hartford, ct\n",
      "Enter location #20: hartford, ct\n",
      "Enter the number of skills or keywords: 2\n",
      "Enter a skill, certification, or other keyword to flag in job postings: python\n",
      "Enter the next skill, certification, or other keyword to flag in job postings: tableau\n",
      "Enter how many pages of search results should be scraped: 50\n"
     ]
    }
   ],
   "source": [
    "# This cell sets all initial variables for gathering and recording data\n",
    "\n",
    "# Two urls need to be set: a base url for the job searches and another for extracting average salaries\n",
    "base_search_url = 'https://www.indeed.com/jobs?q='\n",
    "base_salary_url = 'https://www.indeed.com/career/'\n",
    "\n",
    "# Set/reset the list where job posting information will be stored.\n",
    "job_postings = []\n",
    "\n",
    "# Inputs: If more or different data is needed, these inputs make it easier to scrape data for a new dataset\n",
    "position = input('Enter the job title to search for: ').strip()\n",
    "num_locations = int(input('Enter the number of locations to search for the job position: ').strip())\n",
    "print('Enter each location as city, state (abbreviate state)')\n",
    "locations_list = []\n",
    "for location in range(num_locations):\n",
    "    city_state = (input('Enter location #{}: '.format(location + 1)).strip())\n",
    "    locations_list.append(city_state)\n",
    "num_skills = int(input('Enter the number of skills or keywords: ').strip())\n",
    "skills_list = []\n",
    "for skill in range(num_skills):\n",
    "    if skill == 0:\n",
    "        skill_keyword = input('Enter a skill, certification, or other keyword to flag in job postings: ').lower()\n",
    "    else:\n",
    "        skill_keyword = input('Enter the next skill, certification, or other keyword to flag in job postings: ').lower()\n",
    "    skills_list.append(skill_keyword)\n",
    "pages = input('Enter how many pages of search results should be scraped: ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6529f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All functions to be called are laid out in this cell\n",
    "\n",
    "# Function for updating the search url when proceeding to the next page of the search results\n",
    "def update_search_url(page):\n",
    "    updated_search_url = f'{base_search_url}{position}&l={location}&start={int(page)*10}'\n",
    "    return updated_search_url\n",
    "\n",
    "# Function that grabs the position's average salary in the current location being searched in\n",
    "def parse_location(listing):\n",
    "    job_location = listing.find('div',{'class':'companyLocation'}).text\n",
    "    if job_location.find(',') != -1: \n",
    "        cutoff = job_location.find(',') + 4  \n",
    "        job_location = job_location[:cutoff]  # Remove zipcodes, since Indeed considers zipcodes invalid\n",
    "    else:\n",
    "        job_location = 'invalid or vague location' # If there's no comma, it's an invalid location\n",
    "    \n",
    "    if job_location == 'invalid or vague location':\n",
    "        location_avg_salary = -1\n",
    "    else:\n",
    "        salary_url = f'{base_salary_url}{position}/salaries/{job_location}'\n",
    "        salary_driver.get(salary_url)\n",
    "        salary_soup = bs(salary_driver.page_source, 'html5lib')\n",
    "        location_avg_salary =  salary_soup.find('div', {'class':'css-15psvrv eu4oa1w0'}).text\n",
    "        removals = [',','$']\n",
    "        for char in removals:\n",
    "            location_avg_salary = location_avg_salary.replace(char,'')\n",
    "    return location, location_avg_salary\n",
    "\n",
    "# Function for parsing pay ranges/exact rates, returning frequency of pay, miniumum pay, and maximum pay.\n",
    "# exact rates will cause minimum pay and maximum pay to be the same.\n",
    "def parse_pay(listing):\n",
    "    try:\n",
    "        pay_range = listing.find('div',{'class':'metadata salary-snippet-container'}).text\n",
    "        pay_range = pay_range.replace(',','')\n",
    "        pay_type = pay_range[-5:].strip() + 'ly'\n",
    "        if pay_range.find('-') == -1:  \n",
    "            pay_max = float(pay_range[1:pay_range.find('a') - 1])\n",
    "            pay_min = float(pay_range[1:pay_range.find('a') - 1])\n",
    "        else: \n",
    "            pay_min = float(pay_range[1:pay_range.find('-') - 1])\n",
    "            pay_max = float(pay_range[pay_range.find('-')+3:pay_range.find('a')-1])\n",
    "    except:\n",
    "        pay_type = 'unlisted'\n",
    "        pay_min = -1\n",
    "        pay_max = -1\n",
    "    return pay_type, pay_min, pay_max\n",
    "\n",
    "# Function for scanning for the skillset \"keyword\" in each post, returning 1 for present and 0 for missing\n",
    "def skill_search(listing):\n",
    "    job_url = 'https://indeed.com/' + listing.find('h2').find('a')['href']\n",
    "    keyword_driver.get(job_url)\n",
    "    time.sleep(2.5) \n",
    "    job_soup = bs(keyword_driver.page_source, 'html5lib')\n",
    "    flag_list = []\n",
    "    for skill in skills_list:\n",
    "        if skill in job_soup.text.lower():\n",
    "            flag = 1\n",
    "        else:\n",
    "            flag = 0\n",
    "        flag_list.append(flag)    \n",
    "    return flag_list\n",
    "                        \n",
    "# Function for examining search results and calling the record function for all information of interest.\n",
    "def search_scrape(url):\n",
    "    search_driver.get(url)\n",
    "    search_driver.implicitly_wait(60)\n",
    "    soup = bs(search_driver.page_source, 'html5lib')\n",
    "    \n",
    "    listings_div = soup.find('div', attrs={'id': 'mosaic-provider-jobcards'})\n",
    "    for listing in listings_div.find('ul'):\n",
    "        try:\n",
    "            position = listing.find('h2').find('a').text\n",
    "            company = listing.find('span',{'class':'companyName'}).text\n",
    "            \n",
    "            # Call parsing function for job location, location's average pay for position\n",
    "            job_location, avg_salary = parse_location(listing)\n",
    "            \n",
    "            # Call parsing function for pay type, min, max\n",
    "            pay_type, pay_min, pay_max = parse_pay(listing)\n",
    "            \n",
    "            # Call skillset keyword search function to confirm whether or not it's present in the job posting's description\n",
    "            skill_flags = skill_search(listing)\n",
    "            \n",
    "            # Append all information gathered to job_postings as a list\n",
    "            job_details = [position, company, job_location, avg_salary, pay_type, pay_min, pay_max]\n",
    "            for flag in skill_flags:\n",
    "                job_details.append(flag)\n",
    "            job_postings.append(job_details)\n",
    "        \n",
    "        # Job Search page contains ads/misc postings that use the same job-card format as the actual job postings.\n",
    "        # Attempting to assign position for these will produce an AttributeError;  continue in this case.\n",
    "        except AttributeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b098f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Scrape\n",
    "for location in locations_list:\n",
    "    for page in range(int(pages)):\n",
    "        search_url = update_search_url(page)\n",
    "        search_scrape(search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879a81d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "columns = ['Job Name','Company','Location','Avg Salary','Pay Type','Pay Min','Pay Max']\n",
    "for skill in skills_list:\n",
    "    columns.append(f'Mentions {skill}')\n",
    "df = pd.DataFrame(job_postings, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34642aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv for future use\n",
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "df.to_csv(f'Indeed_Job_Scrape_Raw_{date_str}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872e106",
   "metadata": {},
   "source": [
    "This concludes the scraping script. Data cleaning and wrangling are performed in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
